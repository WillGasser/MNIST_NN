{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will Gasser | 2/20/2025 | MNIST NN\n",
    "\n",
    "This is a simple NN with back propogation to formalize my understanding of ML and NN concepts.\n",
    "\n",
    "I am limiting myself to having imports: numpy, random, time (just for stats), pandas (just for data read), and matplotlib for plotting \n",
    "\n",
    "Let us see how fast I can complete this, oh and no LLMs for any functions, I will note if I use it for syntax.\n",
    "\n",
    "Init commit now: 2/20/2025 @ 9:14 EST\n",
    "\n",
    "Finish: 2/23/2025 @ 9:17 EST \n",
    "\n",
    "I used LLMS for math notation and used many videos for helping explain the chain rule. \n",
    "Lots of time went into figuring this out I am so ready to go back to using libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = np.array(pd.read_csv('mnist_train.csv'))\n",
    "TEST_DATA = np.array(pd.read_csv('mnist_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA FUNCTIONS\n",
    "\n",
    "DATA_SIZE = np.shape(TRAIN_DATA)\n",
    "BATCH_INDICES = list(range(0, DATA_SIZE[0]))\n",
    "\n",
    "def shuffle_batch() -> None:\n",
    "    random.shuffle(BATCH_INDICES)\n",
    "    \n",
    "def get_one_example(data: np.ndarray) -> np.ndarray:\n",
    "    return data \n",
    "\n",
    "def get_label(sample: np.ndarray) -> int:\n",
    "    return int(sample[0])\n",
    "\n",
    "def get_features(sample: np.ndarray) -> np.ndarray:\n",
    "    return sample[1:][np.newaxis, :] / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIGMOID(Z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def SIGMOID_DERIVATIVE(a: np.ndarray) -> np.ndarray:\n",
    "    return a * (1 - a)\n",
    "\n",
    "def FORWARD_PASS(input_activation: np.ndarray, weights: list, bias: list) -> list:\n",
    "    activations = [input_activation]\n",
    "    a = input_activation\n",
    "    for W, b in zip(weights, bias):\n",
    "        z = np.dot(a, W) + b\n",
    "        a = SIGMOID(z)\n",
    "        activations.append(a)\n",
    "    return activations\n",
    "\n",
    "def UPDATE_PARAMETERS(weights: list, bias: list, weight_gradients: list, bias_gradients: list, learning_rate: float, batch_size: int):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] = weights[i] - learning_rate * (weight_gradients[i] / batch_size)\n",
    "        bias[i] = bias[i] - learning_rate * (bias_gradients[i] / batch_size)\n",
    "    return weights, bias\n",
    "\n",
    "def DELTA(old_delta: np.ndarray, weights_vec: np.ndarray, activation: np.ndarray):\n",
    "    return np.dot(old_delta, weights_vec.T) * SIGMOID_DERIVATIVE(activation)\n",
    "\n",
    "def W_GRADIENT(activation_prev: np.ndarray, delta: np.ndarray) -> np.ndarray:\n",
    "    return np.dot(activation_prev.T, delta)\n",
    "\n",
    "def B_GRADIENT(delta: np.ndarray) -> np.ndarray:\n",
    "    return np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "def STOCHASTIC_GRADIENT(activation_vectors: list, weights: list, labels: np.ndarray):\n",
    "    activations_all = [np.vstack(layer) for layer in zip(*activation_vectors)]\n",
    "    weight_gradients = [None] * len(weights)\n",
    "    bias_gradients = [None] * len(weights)\n",
    "    a_output = activations_all[-1]\n",
    "    delta = (a_output - labels) * SIGMOID_DERIVATIVE(a_output)\n",
    "    weight_gradients[-1] = W_GRADIENT(activations_all[-2], delta)\n",
    "    bias_gradients[-1] = B_GRADIENT(delta)\n",
    "    for l in reversed(range(1, len(weights))):\n",
    "        delta = DELTA(delta, weights[l], activations_all[l])\n",
    "        weight_gradients[l - 1] = W_GRADIENT(activations_all[l - 1], delta)\n",
    "        bias_gradients[l - 1] = B_GRADIENT(delta)\n",
    "    return weight_gradients, bias_gradients\n",
    "\n",
    "def BACK_PROPOGATION(activation_vectors: list, weights: list, bias: list, learning_rate: float, batch_size: int, labels: np.ndarray):\n",
    "    weight_gradients, bias_gradients = STOCHASTIC_GRADIENT(activation_vectors, weights, labels)\n",
    "    weights, bias = UPDATE_PARAMETERS(weights, bias, weight_gradients, bias_gradients, learning_rate, batch_size)\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data_size: int, epochs: int, batch_size: int, learning_rate: float, data: np.ndarray, weights: list, bias: list, internal_test: bool, logs: bool) -> None:\n",
    "    BATCHES = data_size // batch_size\n",
    "    if logs:\n",
    "        print(f'Commencing Training with:\\nEpochs: {epochs}\\nBatches: {BATCHES}\\nBatch Size: {batch_size}')\n",
    "    total_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_time = time.time()\n",
    "        for batch in range(BATCHES):\n",
    "            labels = np.zeros((batch_size, 10))\n",
    "            activation_vectors = []\n",
    "            for batch_index in range(batch_size):\n",
    "                sample_index = batch * batch_size + batch_index\n",
    "                sample = data[sample_index]\n",
    "                label = get_label(sample)\n",
    "                features = get_features(sample)\n",
    "                labels[batch_index, label] = 1\n",
    "                activation_vectors.append(FORWARD_PASS(features, weights, bias))\n",
    "            weights, bias = BACK_PROPOGATION(activation_vectors, weights, bias, learning_rate, batch_size, labels)\n",
    "        if logs:\n",
    "            print(f'EPOCH {epoch} complete in {time.time() - epoch_time:.2f}(s)')\n",
    "            if internal_test: testing(TEST_DATA, weights, bias, logs)\n",
    "    if logs:\n",
    "        print(f'TRAINING COMPLETE in time: {time.time() - total_time:.2f}(s)')\n",
    "\n",
    "def testing(data: np.ndarray, weights: list, bias: list, logs: bool):\n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    correct_predictions = 0\n",
    "    total_samples = len(data)\n",
    "    for idx in indices:\n",
    "        sample = data[idx]\n",
    "        features = get_features(sample)\n",
    "        actual_label = get_label(sample)\n",
    "        activations = FORWARD_PASS(features, weights, bias)\n",
    "        predicted_label = np.argmax(activations[-1])\n",
    "        if predicted_label == actual_label:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    if logs:\n",
    "        print(f'Accuracy: {accuracy * 100:.2f}% ({correct_predictions} / {total_samples})')\n",
    "        print('---------------------------------')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "###################################################################\n",
    "### THIS CELL SETS THE MODEL WEIGHTS/BIAS BACK TO RANDOM VALUES ###\n",
    "###################################################################\n",
    "'''\n",
    "\n",
    "# MODEL INSTANTIATION\n",
    "\n",
    "def initialize_model():\n",
    "    weights784 = np.random.uniform(low=-1.0, high=1.0, size=(784, 256))\n",
    "    bias256 = np.zeros((1,256))\n",
    "    weights256 = np.random.uniform(low=-1.0, high=1.0, size=(256, 128))\n",
    "    bias128 = np.zeros((1,128))\n",
    "    weights128 = np.random.uniform(low=-1.0, high=1.0, size=(128, 64))\n",
    "    bias64 = np.zeros((1,64))\n",
    "    weights64 = np.random.uniform(low=-1.0, high=1.0, size=(64, 10))\n",
    "    bias10 = np.zeros((1,10))\n",
    "    return [weights784, weights256, weights128, weights64], [bias256, bias128, bias64, bias10]\n",
    "\n",
    "\n",
    "WEIGHTS, BIAS = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing, shuffling batch of size 59999.\n",
      "Data shuffling complete, took 0.02 seconds.\n",
      "Commencing Training with:\n",
      "Epochs: 5\n",
      "Batches: 11999\n",
      "Batch Size: 5\n",
      "EPOCH 0 complete in 13.99(s)\n",
      "Accuracy: 87.53% (8752 / 9999)\n",
      "---------------------------------\n",
      "EPOCH 1 complete in 13.97(s)\n",
      "Accuracy: 90.99% (9098 / 9999)\n",
      "---------------------------------\n",
      "EPOCH 2 complete in 13.46(s)\n",
      "Accuracy: 92.41% (9240 / 9999)\n",
      "---------------------------------\n",
      "EPOCH 3 complete in 13.26(s)\n",
      "Accuracy: 93.15% (9314 / 9999)\n",
      "---------------------------------\n",
      "EPOCH 4 complete in 14.31(s)\n",
      "Accuracy: 93.66% (9365 / 9999)\n",
      "---------------------------------\n",
      "TRAINING COMPLETE in time: 72.91(s)\n"
     ]
    }
   ],
   "source": [
    "# DATA PRE-PROCESSING\n",
    "print(f'Data processing, shuffling batch of size {len(BATCH_INDICES)}.')\n",
    "start = time.time()\n",
    "shuffle_batch()\n",
    "shuffle_time = time.time() - start\n",
    "print(f'Data shuffling complete, took {shuffle_time:.2f} seconds.')\n",
    "\n",
    "# TRAINING BLOCK\n",
    "DATA_SIZE = len(TRAIN_DATA)\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 5\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "training(DATA_SIZE, EPOCHS, BATCH_SIZE, LEARNING_RATE, TRAIN_DATA, WEIGHTS, BIAS, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, Accuracy: 89.92%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.03\u001b[39m, \u001b[38;5;241m0.01\u001b[39m):\n\u001b[0;32m     13\u001b[0m     test_weights, test_bias \u001b[38;5;241m=\u001b[39m weights, bias\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_DATA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     acc \u001b[38;5;241m=\u001b[39m testing(TEST_DATA, test_weights, test_bias, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m     results[lr] \u001b[38;5;241m=\u001b[39m acc\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(data_size, epochs, batch_size, learning_rate, data, weights, bias, internal_test, logs)\u001b[0m\n\u001b[0;32m     15\u001b[0m         features \u001b[38;5;241m=\u001b[39m get_features(sample)\n\u001b[0;32m     16\u001b[0m         labels[batch_index, label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 17\u001b[0m         activation_vectors\u001b[38;5;241m.\u001b[39mappend(\u001b[43mFORWARD_PASS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m     weights, bias \u001b[38;5;241m=\u001b[39m BACK_PROPOGATION(activation_vectors, weights, bias, learning_rate, batch_size, labels)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs:\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36mFORWARD_PASS\u001b[1;34m(input_activation, weights, bias)\u001b[0m\n\u001b[0;32m      9\u001b[0m a \u001b[38;5;241m=\u001b[39m input_activation\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m W, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weights, bias):\n\u001b[1;32m---> 11\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m     12\u001b[0m     a \u001b[38;5;241m=\u001b[39m SIGMOID(z)\n\u001b[0;32m     13\u001b[0m     activations\u001b[38;5;241m.\u001b[39mappend(a)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Okay let's find optimal hyper parameters, this is going to take a long time\n",
    "\n",
    "best_lr = None\n",
    "best_accuracy = 0\n",
    "results = {}\n",
    "DATA_SIZE = len(TRAIN_DATA)\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 2 # why do I even do stochastic if I just average two, I am a perfectionist what a waste of time cry cry cry\n",
    "\n",
    "weights, bias = initialize_model() # let's use the same model instantiation across each iteration\n",
    "\n",
    "for lr in np.arange(0.01, 0.03, 0.01):\n",
    "    test_weights, test_bias = weights, bias\n",
    "    \n",
    "    training(DATA_SIZE, EPOCHS, BATCH_SIZE, lr, TRAIN_DATA, test_weights, test_bias, False, False)\n",
    "    acc = testing(TEST_DATA, test_weights, test_bias, False)\n",
    "    results[lr] = acc\n",
    "    \n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_lr = lr\n",
    "        \n",
    "    print(f'Learning Rate: {lr:.2f}, Accuracy: {acc*100:.2f}%')\n",
    "print(f'Our best learning rate was {best_lr:.2} with an accuracy of {best_accuracy}')\n",
    "\n",
    "learning_rates = list(results.keys())\n",
    "accuracies = [results[lr]*100 for lr in learning_rates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mlearning_rates\u001b[49m, accuracies)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning rate accuracy with epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learning_rates' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(learning_rates, accuracies)\n",
    "plt.xlabel(f'Learning rate accuracy with epochs: {EPOCHS}, batch size: {BATCH_SIZE}')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(min(accuracies), 100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
